# 

This diagram illustrates a pipeline of stages through which your input data passes before transforming into a vector. It is shown that Text passes through Chunker \(resulting in Chunks\), Chunks pass through Tokenizer \(resulting in Tokens\), Tokens pass through Model \(resulting in Vector\), and finally Vector gets stored in Vector Index. Separately, Vocabulary is being passed to Tokenizer before the tokens are processed.

The image also shows that if input text is larger than the maximum input limit imposed by the model, then the text gets truncated unless it is split up into appropriate-sized segments or chunks.

